{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6167a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMChain import *\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "703b8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cfba572",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "gemini_client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "664dcb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LLMChain:VideoGeminiLink System prompt: You are an intelligent assistant that helps organize and plan video edits based on a user prompt and a provided video. Your job is to deeply understand the user's intent and generate a clean, structured list of video edits with precise timestamps or durations.\n",
      "\n",
      "Your output **must** be a **structured JSON list of edits**, with each entry including:\n",
      "\n",
      "* `edit_type`: (chosen from the provided list of valid edits)\n",
      "* `timestamp`: the start time in `HH:MM:SS` format (or frame index if requested)\n",
      "* `duration`: how long the edit should last (`seconds` or `end_time`)\n",
      "* `reasoning`: a short note explaining why this edit is appropriate at that point\n",
      "* *(optional)* `priority`: \"high\", \"medium\", or \"low\", depending on impact\n",
      "\n",
      "Use the video content (scenes, objects, movement, audio cues, speaker tone, etc.) and blend it with the user's prompt to infer where and what types of edits make the most sense.\n",
      "\n",
      "You may segment the video into logical scenes before applying user instructions if needed.\n",
      "\n",
      "**You must:**\n",
      "\n",
      "* Interpret vague prompts by analyzing visual/audio patterns (e.g. if the user says \"make it cinematic\", you may add slow zooms, LUTs, or transitions where impactful moments occur).\n",
      "* Never assume edit types not listed in the allowed edits.\n",
      "* If the user's request is unclear, use common sense and aesthetic principles to interpret intent.\n",
      "\n",
      "**User will provide:**\n",
      "\n",
      "* A prompt (e.g. \"Make it look like a horror trailer\" or \"Cut all boring pauses\")\n",
      "* A video file or stream\n",
      "* A predefined list of valid `edit_type`s (e.g., `\"cut\"`, `\"fade_in\"`, `\"color_grade\"`, `\"slow_motion\"`, etc.)\n",
      "\n",
      "**Final Output Format Example:**\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"edit_type\": \"cut\",\n",
      "    \"timestamp\": \"00:00:12\",\n",
      "    \"duration\": \"00:00:03\",\n",
      "    \"reasoning\": \"Silence detected; cut improves pacing.\",\n",
      "    \"priority\": \"high\"\n",
      "  },\n",
      "  {\n",
      "    \"edit_type\": \"color_grade\",\n",
      "    \"timestamp\": \"00:01:25\",\n",
      "    \"duration\": \"00:00:15\",\n",
      "    \"reasoning\": \"Dramatic lighting enhances horror effect.\",\n",
      "    \"priority\": \"medium\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Only return the JSON list, no extra commentary.\n"
     ]
    }
   ],
   "source": [
    "ai_chain = LLMChain(\n",
    "    \"SunFish Editor\",\n",
    "    [\n",
    "        VideoGeminiLink(\n",
    "            gemini_client, \"gemini-2.5-flash\", open(\"video_sys_prompt.txt\").read()\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1f1e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LLMChain:Starting generation for LLM Chain: SunFish Editor\n",
      "INFO:LLMChain:Processing link at idx: 0\n",
      "INFO:LLMChain:VideoGeminiLink PROMPT: User: I want a faster paced video with more flashiness\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH8_gPP3m9QyEzf-SuoI-jZyGCtsaZYkd0rWtXbhtNamMlHa3uQkZu-6dkWB0BVsVdi7ZEEgcLWqHzPOinAr7tTN19Tcllk3wWnp2cXqAew&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH8_gPP3m9QyEzf-SuoI-jZyGCtsaZYkd0rWtXbhtNamMlHa3uQkZu-6dkWB0BVsVdi7ZEEgcLWqHzPOinAr7tTN19Tcllk3wWnp2cXqAew&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mai_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUser: I want a faster paced video with more flashiness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvideo.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\SunFish-Video-Editor\\LLMChain.py:121\u001b[39m, in \u001b[36mLLMChain.forward\u001b[39m\u001b[34m(self, prompt, vid_path)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m vid_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m         logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo video passed but LLMChain has VideoGeminiLink\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28mself\u001b[39m.next_layer_input = \u001b[43mlink\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext_layer_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(link, BaseLLMLink):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m.next_layer_input = link.forward(\u001b[38;5;28mself\u001b[39m.next_layer_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\SunFish-Video-Editor\\LLMChain.py:299\u001b[39m, in \u001b[36mVideoGeminiLink.forward\u001b[39m\u001b[34m(self, _prompt, _video_path)\u001b[39m\n\u001b[32m    295\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVideoGeminiLink PROMPT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    297\u001b[39m vid_file = \u001b[38;5;28mself\u001b[39m.client.files.upload(file=_video_path)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m response = \u001b[43mclient\u001b[49m.models.generate_content(model=\u001b[38;5;28mself\u001b[39m.model, contents=[vid_file, _prompt], config=genai.types.GenerateContentConfig(system_instruction=\u001b[38;5;28mself\u001b[39m.system_prompt))\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.client.files.delete(vid_file.name)\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.text\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "result = ai_chain.forward(\"User: I want a faster paced video with more flashiness\", \"video.mp4\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
